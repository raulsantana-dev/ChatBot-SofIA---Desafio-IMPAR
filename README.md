ğŸ§  Chatbot RAG Local com MemÃ³ria e Upload de Documentos
ğŸ“Œ VisÃ£o Geral

Este projeto implementa um chatbot AI local com RAG (Retrieval Augmented Generation) capaz de responder perguntas com base em:

Documentos enviados (PDF, CSV, Excel)

ConteÃºdo da web via scraping

MemÃ³ria da conversa

Pesquisa vetorial com PGVector

LLM local via Ollama

Tudo roda 100% local usando Docker Compose, sem depender de APIs externas.

ğŸ— Arquitetura
UsuÃ¡rio (n8n Chat)
      â†“
RequisiÃ§Ã£o HTTP
      â†“
API Python (Litestar)
      â†“
- Embeddings (Ollama local)
- PGVector (PostgreSQL)
- RecuperaÃ§Ã£o RAG
- MemÃ³ria da Conversa
- Resposta via LLM (Ollama)


ServiÃ§os:

n8n â†’ Interface de chat e orquestraÃ§Ã£o

api â†’ Backend (Litestar + LangChain)

postgres â†’ Banco vetorial com PGVector

ollama â†’ LLM local e embeddings

adminer â†’ Interface de banco

ğŸš€ Funcionalidades
Chat

Suporte a mÃºltiplos usuÃ¡rios

MemÃ³ria de conversa por usuÃ¡rio

Respostas contextuais

LLM totalmente local

Upload de Documentos

Suporta:

PDF

CSV

Excel

Fluxo:

Enviar arquivo pelo chat (n8n)

Extrair texto

Gerar embeddings localmente

Armazenar no PostgreSQL (PGVector)

Usado para busca semÃ¢ntica

Web Scraping

Endpoint:

POST /scrape


Rastreia URL configurada

Limpa HTML e converte em texto

Gera embeddings

Armazena em PGVector

ğŸ§  MemÃ³ria

MemÃ³ria de curto prazo por usuÃ¡rio, permitindo contexto independente:

user_id = "raul"
user_id = "joao"
user_id = "empresa_x"


Permite perguntas de acompanhamento como:

"Explique melhor"
"Resuma isso"
"O que eu enviei antes?"

ğŸ—‚ Estrutura do Projeto
chatbot-rag/
â”‚
â”œâ”€â”€ api/
â”‚   â”œâ”€â”€ routes/
â”‚   â”‚   â”œâ”€â”€ chat.py
â”‚   â”‚   â””â”€â”€ scrape.py
â”‚   â”‚
â”‚   â”œâ”€â”€ services/
â”‚   â”‚   â”œâ”€â”€ rag_service.py
â”‚   â”‚   â”œâ”€â”€ memory_service.py
â”‚   â”‚   â”œâ”€â”€ embedding_service.py
â”‚   â”‚   â””â”€â”€ file_service.py
â”‚   â”‚
â”‚   â”œâ”€â”€ database/
â”‚   â”‚   â””â”€â”€ postgres.py
â”‚   â”‚
â”‚   â””â”€â”€ app.py
â”‚
â”œâ”€â”€ docker-compose.yml
â”œâ”€â”€ Dockerfile
â””â”€â”€ requirements.txt

ğŸ³ Executando Localmente
1. Clonar repositÃ³rio
git clone <repo>
cd chatbot-rag

2. Subir os serviÃ§os
docker compose up --build


URLs:

ServiÃ§o	URL
API	http://localhost:8000

n8n	http://localhost:5678

Adminer	http://localhost:8080

Ollama	http://localhost:11434
ğŸ¤– Modelos

ApÃ³s iniciar os containers:

docker exec -it ollama ollama pull llama3
docker exec -it ollama ollama pull nomic-embed-text

ğŸ’¬ Endpoints Principais
POST /chat

Enviar mensagem ou arquivo.

Texto:

{
  "message": "O que Ã© inteligÃªncia artificial?",
  "user_id": "raul"
}


Com arquivo:
Enviar via multipart/form-data com campos: message, user_id, file.

POST /scrape

Dispara scraping e armazenamento de embeddings:

POST http://localhost:8000/scrape

ğŸ§ª Exemplo curl

Chat:

curl -X POST http://localhost:8000/chat \
-H "Content-Type: application/json" \
-d '{"message":"O que Ã© IA?","user_id":"raul"}'


Upload de arquivo:

curl -X POST http://localhost:8000/chat \
-F "message=Resuma este documento" \
-F "user_id=raul" \
-F "file=@file.pdf"

ğŸ‘¨â€ğŸ’» Autor

Desenvolvido por Raul Santana Santos de Araujo como parte do Desafio TÃ©cnico proposto pela IMPAR.
