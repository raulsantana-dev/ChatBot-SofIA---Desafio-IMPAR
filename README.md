# ğŸ§  Chatbot RAG Local com MemÃ³ria e Upload de Documentos

## ğŸ“Œ VisÃ£o Geral

Este projeto implementa um **chatbot AI local com RAG (Retrieval Augmented Generation)** capaz de responder perguntas com base em:

- Documentos enviados (PDF, CSV, Excel)  
- ConteÃºdo da web via scraping  
- MemÃ³ria da conversa  
- Pesquisa vetorial com PGVector  
- LLM local via Ollama  

Tudo roda **100% local usando Docker Compose**, sem depender de APIs externas.

## ğŸ— Arquitetura

UsuÃ¡rio (n8n Chat) â†’ RequisiÃ§Ã£o HTTP â†’ API Python (Litestar)  

ServiÃ§os e funcionalidades da API:

- Embeddings (Ollama local)  
- PGVector (PostgreSQL)  
- RecuperaÃ§Ã£o RAG  
- MemÃ³ria da Conversa  
- Resposta via LLM (Ollama)  

ServiÃ§os do projeto:

- **n8n** â†’ Interface de chat e orquestraÃ§Ã£o  
- **api** â†’ Backend (Litestar + LangChain)  
- **postgres** â†’ Banco vetorial com PGVector  
- **ollama** â†’ LLM local e embeddings  
- **adminer** â†’ Interface de banco  

## ğŸš€ Funcionalidades

### Chat

- Suporte a mÃºltiplos usuÃ¡rios  
- MemÃ³ria de conversa por usuÃ¡rio  
- Respostas contextuais  
- LLM totalmente local  

### Upload de Documentos

- Suporta PDF, CSV e Excel  
- Fluxo:
  1. Enviar arquivo pelo chat (n8n)  
  2. Extrair texto  
  3. Gerar embeddings localmente  
  4. Armazenar no PostgreSQL (PGVector)  
  5. Usado para busca semÃ¢ntica  

### Web Scraping

- Endpoint: `POST /scrape`  
- Rastreia URL configurada  
- Limpa HTML e converte em texto  
- Gera embeddings  
- Armazena em PGVector  

## ğŸ§  MemÃ³ria

MemÃ³ria de curto prazo por usuÃ¡rio, permitindo contexto independente

## ğŸ—‚ Estrutura do Projeto

- api/
  - routes/
    - chat.py
    - scrape.py
  - services/
    - rag_service.py
    - memory_service.py
    - embedding_service.py
  - database/
    - postgres.py
  - app.py
- docker-compose.yml
- Dockerfile
- requirements.txt
- README.md

## ğŸ³ Executando Localmente

1. Clonar repositÃ³rio:

bash
git clone <https://github.com/raulsantana-dev/ChatBot-SofIA---Desafio-IMPAR>
cd chatbot-rag

2. Subir os serviÃ§os:
docker compose up 

3. Baixar modelos Ollama:

docker exec -it ollama ollama pull llama3
docker exec -it ollama ollama pull nomic-embed-text

## ğŸ’¬ Endpoints Principais
POST /chat

Enviar mensagem ou arquivo.

Exemplo de texto:

{
  "message": "O que Ã© inteligÃªncia artificial?",
  "user_id": "raul"
}


Exemplo com arquivo:
Enviar via multipart/form-data com campos: message, user_id, file.

POST /scrape

Dispara scraping e armazenamento de embeddings:

POST http://localhost:8000/scrape


## ğŸ‘¨â€ğŸ’» Autor

Desenvolvido por Raul Santana Santos de Araujo como parte do Desafio TÃ©cnico proposto pela IMPAR.